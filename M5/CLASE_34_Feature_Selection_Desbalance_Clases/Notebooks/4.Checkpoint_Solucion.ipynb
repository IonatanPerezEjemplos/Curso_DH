{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "        \n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    args = \"conda-forge\"\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\" {args}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint - Desbalance de clases + Feature Selection\n",
    "\n",
    "En este checkpoint trabajaremos con el dataset de préstamos crediticios 'loans.csv'.\n",
    "\n",
    "En la primera parte combinaremos los elementos vistos en la práctica guiada de clases desbalanceadas para resamplear el data set de entrenamiento.\n",
    "\n",
    "Luego implementaremos un modelo de regresión logística combinado con feature selection por eliminación recursiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.metrics import accuracy_score,plot_confusion_matrix,roc_auc_score, classification_report, confusion_matrix, precision_recall_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans = pd.read_csv('../Data/loans.csv',low_memory=False)\n",
    "loans.dropna(inplace=True)\n",
    "loans.reset_index(drop=True,inplace=True)\n",
    "loans['bad_loans'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train test split \n",
    "\n",
    "Hagamos un train test split estratificado por la variable target (bad loans). \n",
    "\n",
    "¿Cuál es el desbalance de clases?\n",
    "\n",
    "¿Cuál es el ratio clase mayoritaria/minoritaria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train,X_test,y_train,y_test=train_test_split(loans.drop(['bad_loans'],axis=1),\\\n",
    "                                               loans['bad_loans'],stratify=loans['bad_loans'],random_state=0)\n",
    "\n",
    "print('Tamaño del training set:')\n",
    "print(X_train.shape)\n",
    "\n",
    "print('\\nBalance de clases:')\n",
    "print(y_train.value_counts(normalize=False))\n",
    "\n",
    "\n",
    "print('\\nBalance de clases norm:')\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print('\\nRatio entre las clases:')\n",
    "print((y_train==1).sum()/(y_train==0).sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SMOTE-NC \n",
    "\n",
    "Implementemos la herramienta SMOTENC para resamplear la clase minoritaria en el dataset de entrenamiento. Para ello debemos identificar previamente las variables categóricas y pasarlas como argumento al instanciar el objeto SMOTENC.\n",
    "\n",
    "Usemos un sampling_strategy=0.5, de modo que la clase mayoritaria tenga el doble de observaciones que la minoritaria.\n",
    "\n",
    "¿Cuál es el balance de clases ahora?\n",
    "\n",
    "¿Cuál es el ratio mayoritaria/minoritaria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols=(X_train.dtypes=='object').values\n",
    "numerical_cols= ~categorical_cols\n",
    "\n",
    "sm=SMOTENC(categorical_features=categorical_cols ,sampling_strategy=0.5)\n",
    "X_train_os,y_train_os=sm.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Balance de clases:')\n",
    "print(y_train_os.value_counts(normalize=False))\n",
    "\n",
    "print('\\nBalance de clases norm:')\n",
    "print(y_train_os.value_counts(normalize=True))\n",
    "\n",
    "print('\\nRatio entre las clases:')\n",
    "print((y_train_os==1).sum()/(y_train_os==0).sum())\n",
    "\n",
    "print('\\nTamaño de Xtrain:')\n",
    "print(X_train_os.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Undersampling\n",
    "\n",
    "A partir del dataset aumentado en el paso anterior balanceemos las clases sampleando aleatoriamente observaciones de la clase mayoritaria. Para esto usemos un RandomUnderSampler con sampling_strategy='majority' de modo que queden las clases balanceadas.\n",
    "\n",
    "¿Cómo quedó el balance de clases?\n",
    "\n",
    "¿Y el tamaño del dataset de entrenamiento?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler=RandomUnderSampler(sampling_strategy='majority');\n",
    "X_train_us,y_train_us=undersampler.fit_resample(X_train_os,y_train_os)\n",
    "\n",
    "print('Balance de clases:')\n",
    "print(y_train_us.value_counts(normalize=False))\n",
    "\n",
    "print('\\nBalance de clases norm:')\n",
    "print(y_train_us.value_counts(normalize=True))\n",
    "\n",
    "print('\\nRatio entre las clases:')\n",
    "print((y_train_us==1).sum()/(y_train_us==0).sum())\n",
    "\n",
    "print('\\nTamaño de Xtrain:')\n",
    "print(X_train_us.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Variables dummy\n",
    "\n",
    "Utilicemos la herramienta OneHotEncoder para llevar a dummies las variables categoricas del training set. Luego transformamos de manera conforme las categóricas del test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variables\n",
    "\n",
    "enc=OneHotEncoder(drop='first',sparse=False)\n",
    "train_dummies=enc.fit_transform(X_train_us.loc[:,categorical_cols])\n",
    "train_dummies=pd.DataFrame(train_dummies)\n",
    "train_dummies.columns=[x for cat_list in enc.categories_ for x in cat_list[1:]]\n",
    "\n",
    "X_train_final=X_train_us.loc[:,numerical_cols].join(train_dummies)\n",
    "\n",
    "test_dummies=enc.transform(X_test.loc[:,categorical_cols])\n",
    "test_dummies=pd.DataFrame(test_dummies)\n",
    "test_dummies.columns=[x for cat_list in enc.categories_ for x in cat_list[1:]]\n",
    "test_dummies.index=X_test.index\n",
    "\n",
    "X_test_final=X_test.loc[:,numerical_cols].join(test_dummies)\n",
    "\n",
    "y_train_final=y_train_us # solo por nomenclatura\n",
    "y_test_final=y_test # solo por nomenclatura\n",
    "\n",
    "print(X_train_final.shape)\n",
    "print(X_test_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Regresión logística\n",
    "\n",
    "Implementemos una regresión logística usando LogisticRegressionCV. \n",
    "\n",
    "¿Qué medida de scoring conviene usar?\n",
    "\n",
    "Veamos las métricas principales de performance en el test set: \n",
    "\n",
    "* matriz de confusión\n",
    "* clasification report\n",
    "* área bajo la curva ROC\n",
    "* área bajo la curva Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_report(X_train,X_test,y_train,y_test):\n",
    "    \n",
    "    scaler=StandardScaler()    \n",
    "        \n",
    "    X_train_sc=scaler.fit_transform(X_train)\n",
    "    X_test_sc=scaler.transform(X_test)\n",
    "        \n",
    "    model=LogisticRegressionCV(scoring='f1')\n",
    "        \n",
    "    model.fit(X_train_sc,y_train)\n",
    "    y_pred=model.predict(X_test_sc)\n",
    "    y_proba=model.predict_proba(X_test_sc)\n",
    "    \n",
    "    print(classification_report(y_test,y_pred))\n",
    "    \n",
    "    print('Area bajo la curva ROC:',np.round(roc_auc_score(y_test,y_proba[:,1]),4))\n",
    "    \n",
    "    precision, recall,threshold=precision_recall_curve(y_test,y_proba[:,1]);\n",
    "\n",
    "    print('Area bajo la curva Precision-Recall:',np.round(auc(recall,precision),4))\n",
    "\n",
    "    plot_confusion_matrix(model,X_test_sc,y_test,cmap='Blues');\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=logistic_regression_report(X_train_final,X_test_final,y_train_final,y_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature selection\n",
    "\n",
    "Utilicen la herramientsa RFECV combinada con una regresión logística para seleccionar el mejor subset de features. Usen una regresión logística con el hiperparámetro de regularización C ajustado en el punto anterior. \n",
    "\n",
    "Lo correcto sería optimizar este parámetro para cada subset de features, es decir hacer dos validaciones cruzadas anidadas (una para RFE y otra para la regresión logística), pero por \n",
    "economía de cómputo lo haremos con el valor de C fijo.\n",
    "\n",
    "¿Cuáles features fueron seleccionadas?\n",
    "\n",
    "¿Cuáles fueron descartadas?\n",
    "\n",
    "¿Cuál es la performance del modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "model = LogisticRegression(C=m.C_[0]) # La penalización óptima podría depender del nro de Features, pero para no sobrecargar el cómputo no optimizaremos C para cada representación de los datos\n",
    "\n",
    "scaler=StandardScaler()    \n",
    "X_train_sc=scaler.fit_transform(X_train_final)\n",
    "X_test_sc=scaler.transform(X_test_final)\n",
    "\n",
    "\n",
    "selector = RFECV(model, scoring='f1',step = 1, cv=skf, verbose=0,n_jobs=3)\n",
    "selector.fit(X_train_sc, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit_transform(X_train_final).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced=X_train_sc[:,selector.support_]\n",
    "X_test_reduced=X_test_sc[:,selector.support_]\n",
    "logistic_regression_report(X_train_reduced,X_test_reduced,y_train_final,y_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features Conservadas:')\n",
    "print(X_train_final.columns[selector.support_])\n",
    "\n",
    "print('\\nFeatures Descartadas')\n",
    "print(X_train_final.columns[~selector.support_])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
